import json
import os
import time
from playwright.sync_api import sync_playwright
from concurrent.futures import ThreadPoolExecutor

def sanitize_filename(filename):
    """Sanitize file name by replacing invalid characters."""
    invalid_chars = ['|', ':', '/', '\\', '?', '*', '<', '>', '"']
    for char in invalid_chars:
        filename = filename.replace(char, "_")
    return filename

def save_as_pdf(page, file_name):
    """Generate PDF using Playwright's built-in method."""
    try:
        page.pdf(path=file_name, format="A4", print_background=True)
        print(f"Saved PDF using Playwright: {file_name}")
    except Exception as e:
        print(f"Failed to save PDF using Playwright: {e}")

def process_link(link, output_dir, retries=3):
    """Process a single link to generate PDFs with retries."""
    attempt = 0
    while attempt < retries:
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)  # Headless mode enabled
                page = browser.new_page()
                page.goto(link, wait_until="domcontentloaded", timeout=60000)

                # Generate filenames
                page_title = sanitize_filename(page.title().replace(" ", "_"))
                file_name_save_as_pdf = os.path.join(output_dir, f"{page_title}_save_as_pdf.pdf")

                # Save as PDF
                save_as_pdf(page, file_name_save_as_pdf)

                browser.close()
                break  # Exit loop if the link was successfully processed
        except Exception as e:
            print(f"Error processing {link} (Attempt {attempt + 1}): {e}")
            attempt += 1
            if attempt >= retries:
                print(f"Failed to process {link} after {retries} attempts.")

def load_links_from_json(file_path):
    """Load links from a JSON file."""
    with open(file_path, 'r') as f:
        data = json.load(f)
        if not isinstance(data, dict) or 'links' not in data or not isinstance(data['links'], list):
            raise ValueError("JSON must contain a 'links' key with a list of URLs.")
        return data['links']

def process_links_in_batches(links, batch_size, output_dir):
    """Process links in smaller batches to avoid overload."""
    with ThreadPoolExecutor(max_workers=5) as executor:
        for i in range(0, len(links), batch_size):
            batch = links[i:i + batch_size]
            executor.map(lambda link: process_link(link, output_dir), batch)

def main():
    json_file_path = "cat.json"
    output_dir = "output_pdfs"  # Directory to save PDFs
    os.makedirs(output_dir, exist_ok=True)

    links = load_links_from_json(json_file_path)
    print(f"Loaded {len(links)} links.")

    # Process the links in batches of 10 for efficiency
    process_links_in_batches(links, batch_size=10, output_dir=output_dir)

if __name__ == "__main__":
    main()
